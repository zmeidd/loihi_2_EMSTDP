{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79fdaeb2-43b7-4634-8525-5b72b20356c2",
   "metadata": {},
   "source": [
    "# Testing Incremental Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd8c98-2fce-42b3-9e39-2f8f95d29fef",
   "metadata": {},
   "source": [
    "Preparing the preprocessed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306a52e0-a9a2-43d2-b672-b7cbd21caa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.model as md\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "\n",
    "import sys\n",
    "# import pyximport; pyximport.install()\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.keras.layers.advanced_activations import ReLU\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from tensorflow.python.ops.gen_math_ops import mod\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, Conv2D, Input, MaxPooling2D, Dense, AveragePooling2D\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "from skimage.transform import resize\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0bd269-7b8f-45ae-ba96-a347576332eb",
   "metadata": {},
   "source": [
    "# Preparing the inputs for loihi2 net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25a7295-3e4b-42a8-b224-c0ce46e114ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "test_size =1000\n",
    "\n",
    "# raw mnist data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train[:train_size]\n",
    "y_train = y_train[:train_size]\n",
    "\n",
    "#prepare pre-trained data from pretrained model\n",
    "model = md.loihi_conv_model()\n",
    "x_train = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a7ee09-458a-41d2-9c41-79cbca88a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4275, 200)\n"
     ]
    }
   ],
   "source": [
    "# prepare incremental learning tasks data\n",
    "task_1 = [0,1]\n",
    "task_2 = [2,3]\n",
    "task_3 = [4,5]\n",
    "task_4 = [6,7]\n",
    "task_5 = [8,9]\n",
    "\n",
    "task_1_data = []\n",
    "task_2_data = []\n",
    "task_3_data = []\n",
    "task_4_data = []\n",
    "task_5_data = []\n",
    "\n",
    "\n",
    "task_1_label = []\n",
    "task_2_label = []\n",
    "task_3_label = []\n",
    "task_4_label = []\n",
    "task_5_label = []\n",
    "\n",
    "\n",
    "full_data = 5*[None]\n",
    "for i in range(len(x_train)):\n",
    "    data =  np.expand_dims(x_train[i],axis =0)\n",
    "    if y_train[i] in task_1:\n",
    "        if len(task_1_data) ==0:\n",
    "            task_1_data = data\n",
    "        else:\n",
    "            task_1_data = np.vstack((task_1_data,data))\n",
    "        task_1_label.append(y_train[i])\n",
    "    elif y_train[i] in task_2:\n",
    "        if len(task_2_data) ==0:\n",
    "            task_2_data = data\n",
    "        else:\n",
    "            task_2_data = np.vstack((task_2_data,data))\n",
    "        task_2_label.append(y_train[i])\n",
    "\n",
    "    elif y_train[i] in task_3:\n",
    "        if len(task_3_data) ==0:\n",
    "            task_3_data = data\n",
    "        else:\n",
    "            task_3_data = np.vstack((task_3_data,data))\n",
    "        task_3_label.append(y_train[i])\n",
    "    elif y_train[i] in task_4:\n",
    "        if len(task_4_data) ==0:\n",
    "            task_4_data = data\n",
    "        else:\n",
    "            task_4_data = np.vstack((task_4_data,data))\n",
    "        task_4_label.append(y_train[i])\n",
    "    else:\n",
    "        if len(task_5_data) ==0:\n",
    "            task_5_data = data\n",
    "        else:\n",
    "            task_5_data = np.vstack((task_5_data,data))\n",
    "        task_5_label.append(y_train[i])\n",
    "\n",
    "\n",
    "print(task_1_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88333ac8-e75f-4178-904b-e37c14b173f5",
   "metadata": {},
   "source": [
    "## check each classes number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c58c63-d38f-4693-84cf-e9ce41719b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = [[task_1_data, task_1_label],[task_2_data, task_2_label],[task_3_data, task_3_label],[task_4_data, task_4_label],[task_5_data, task_5_label]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75bf53-2799-4125-a5b4-462dc3e52e50",
   "metadata": {},
   "source": [
    "### Create Loihi Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8189c-6a8a-4185-b1a2-a598b05e2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loihi_net import loihi2_net\n",
    "net = loihi2_net([200,100,10],time_steps = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e9949-7034-4155-aba8-0984d286ac5d",
   "metadata": {},
   "source": [
    "### you should change environment settings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b6c1a-2fb5-4112-82f7-cea8b7d8ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!SLURM=1 LOIHI_GEN=N3B3 PARTITION=oheogulch_2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4888e-e556-4a58-82c5-93060fdae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [x_train[:10],y_train[:10]]\n",
    "w_h, w_o=net.train_loihi_network(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b7a7a-1e0a-4b16-bb90-d5a17414d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chick_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea63d2-9bc6-46c2-90c6-a3ba14441d75",
   "metadata": {},
   "source": [
    "### main incremental learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca6acdc1-ab5c-4502-b35a-91dbdddf2626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "accuracy = []\n",
    "epochs = 10\n",
    "current_task_size = 500\n",
    "memory_size = 200\n",
    "memory_set = []\n",
    "test_size = 1000\n",
    "final_acc = []\n",
    "for i in range(len(full_data)):\n",
    "    current_set = full_data[i]\n",
    "    for e in range(epochs):\n",
    "        #train loop with experience replay\n",
    "        for j in range(len(current_set[0])//current_task_size):\n",
    "            #train current set\n",
    "            set_1 = [current_set[0][j*current_task_size:(j+1)*current_task_size]]\n",
    "            w_h, w_o=net.train_loihi_network(set_1)\n",
    "            del net\n",
    "            print(j)\n",
    "            net = loihi2_net(dim =[200,100,10],w_h =w_h, w_o= w_o)\n",
    "            #train memory set\n",
    "            if len(memory_set)!=0:\n",
    "                pass\n",
    "                 w_h, w_o=net.train_loihi_network(memory_set)\n",
    "                 del net\n",
    "    #memory setttings\n",
    "    if len(memory_set) ==0:\n",
    "        x = current_set[0][:memory_size]\n",
    "        y = current_set[1][:memory_size]\n",
    "        print(len(y))\n",
    "    else:\n",
    "        x = np.vstack((memory_set[0],current_set[0][:memory_size]))\n",
    "        y = memory_set[1].append(current_set[1])\n",
    "        memory_set = [x,y]\n",
    "        acc =net.test_loihi(memory_set)\n",
    "        #test memory set\n",
    "        \n",
    "#Testing \n",
    "test_set = [x_train[:test_size], y_train[:test_size]]\n",
    "acc =net.test_loihi(test_set)\n",
    "print(\"average acc of 5 tasks is \", acc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ea512-6036-4bef-9ead-8b646122b0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
